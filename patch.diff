diff --git a/README.md b/README.md
index 89431be3c3db65ef4ad309b437d13ef39963f5ef..78c5cce76d563cea42c07f9abea981fcafddf59e 100644
--- a/README.md
+++ b/README.md
@@ -13,51 +13,55 @@ enabled by default:
 
 ```bash
 cargo run
 ```
 
 For release builds use `cargo build --release` followed by the generated binary
 in `target/release/polars_parquet_learning`.
 
 ## Example workflow
 
 1. Start the application with `cargo run`.
 2. Enter the path to a Parquet file in the **File** field, or drag a file onto the window.
 3. Choose one of the available operations.
    * **Read**: load an existing file into a `DataFrame` using the lazy API.
    * **Modify**: convert rows to typed `Record`s and append `!` to each name.
    * **Write**: write the current in-memory `DataFrame` back to the path in **File**.
    * **Create**: define a schema and rows in the UI then save to the **Save** path.
    * **Partition**: split the loaded `DataFrame` by one or more columns and
      write each combination to nested folders under the **Save** path.
    * **Query**: filter the file by a name prefix.
 4. Use the **Save** field to specify where newly created or partitioned data
    should be written.
 5. Click **Run** to perform the selected action. Status is printed to stdout.
 
 The helper functions live in [`src/parquet_examples.rs`](src/parquet_examples.rs)
-and are covered by the `round_trip` unit test.
+and are covered by the `round_trip` unit test. Small example datasets such as
+[`numeric.parquet`](tests/fixtures/numeric.parquet),
+[`temporal.parquet`](tests/fixtures/temporal.parquet) and
+[`nested.parquet`](tests/fixtures/nested.parquet) are bundled under
+`tests/fixtures` for experimentation.
 
 ## Polars and Parquet notes
 
 Polars stores data in columnar Arrow memory which is efficient for analytical
 workloads.  Large datasets may require substantial RAM; prefer the lazy API when
 possible.  Parquet enforces strict schemas so field names and types must match
 when writing new files.  The `polars` crate already includes a mature Parquet
 implementation so the lower level `parquet` dependency is optional here.
 
 ## GUI overview
 
 The interface is implemented using
 [eframe](https://docs.rs/eframe/latest/eframe/) and
 [egui](https://docs.rs/egui/latest/egui/).  When launched a small window is
 displayed containing a text field, radio buttons for the operation and a **Run**
 button. Files can also be dragged onto the window to automatically populate the
 path. All heavy lifting is handled by Polars; the GUI simply wires the chosen
 action to the example functions.
 
 The application includes a simple chart viewer using
 [`egui_plot`](https://crates.io/crates/egui_plot). A drop-down in the preview
 panel lets you pick one or two numeric columns and choose between histogram,
 line, scatter or box plots for that data.
 
 A small statistics table summarising the loaded `DataFrame` is also shown in the
@@ -87,51 +91,51 @@ details and tests for each use case.
 
 ## Creating a DataFrame from scratch
 
 Choose the **Create** mode in the GUI to build a brand new dataset. Add column
 names and pick their types from a drop-down list, then enter one or more rows of
 data. Providing a path in the **Save** field will write the result as a Parquet
 file when **Run** is clicked. The helper function
 [`create_dataframe`](src/parquet_examples.rs) shows how to accomplish the same
 programmatically.
 
 Allowed column types are `int`/`i64`, `str`/`string`, `float`/`f64`,
 `bool`/`boolean`, `date`, `datetime` and `time`.
 
 ## Command line
 
 Running the binary with additional arguments invokes a CLI instead of the GUI.
 Each subcommand mirrors one of the operations available in the application.
 
 ```
 cargo run -- <SUBCOMMAND> [OPTIONS]
 ```
 
 Example reading a file:
 
 ```
-cargo run -- read path/to/file.parquet
+cargo run -- read tests/fixtures/numeric.parquet
 ```
 
 When writing files the `write` subcommand accepts an optional
 `--compression` flag which defaults to `snappy`:
 
 ```
 cargo run -- write input.parquet output.parquet --compression gzip
 ```
 
 ## Benchmarks
 
 Running `cargo bench` builds a small benchmark comparing the old per-file
 collection approach against using `scan_parquet` over a directory pattern.
 
 ## Converting XML to normalized Parquet tables
 
 The CLI can also transform a small hierarchical XML format into multiple
 normalized Parquet tables. The input must contain a `<root>` element with any
 combination of `<template>`, `<message>` and `<repository>` children. Templates
 may include nested `<field>` elements while messages can contain one or more
 `<part>` nodes.
 
 Use the following command to perform the conversion:
 
 ```
diff --git a/src/parquet_examples.rs b/src/parquet_examples.rs
index ab775aae5e4051df3de885397aa6b83904bd5e0a..8e69eac8582b356a7398326400ad34270c4a0aa5 100644
--- a/src/parquet_examples.rs
+++ b/src/parquet_examples.rs
@@ -760,100 +760,121 @@ pub fn filter_count(path: &str, exprs: &[String]) -> Result<usize> {
 /// column information among other details.
 pub fn read_parquet_metadata(path: &str) -> Result<parquet::file::metadata::ParquetMetaData> {
     use parquet::file::reader::{FileReader, SerializedFileReader};
 
     let file = File::open(path)?;
     let reader = SerializedFileReader::new(file)?;
     Ok(reader.metadata().clone())
 }
 
 /// Write small example datasets illustrating each operation to `dir`.
 pub fn write_example_data(dir: &str) -> Result<()> {
     use std::path::Path;
 
     let base = Path::new(dir);
     std::fs::create_dir_all(base)?;
 
     let folders = [
         "read",
         "modify",
         "write",
         "partition",
         "query",
         "xml",
         "xml_dynamic",
         "correlation",
+        "datasets",
     ];
     for f in &folders {
         std::fs::create_dir_all(base.join(f))?;
     }
 
     // Read example
     let df = df!("id" => &[1i64, 2], "name" => &["alice", "bob"])?;
     create_and_write_parquet(&df, base.join("read/data.parquet").to_str().unwrap())?;
 
     // Modify example
     let df = df!("id" => &[1i64, 2], "name" => &["a", "b"])?;
     create_and_write_parquet(&df, base.join("modify/input.parquet").to_str().unwrap())?;
 
     // Write example
     let df = df!("id" => &[10i64, 20], "name" => &["foo", "bar"])?;
     create_and_write_parquet(&df, base.join("write/input.parquet").to_str().unwrap())?;
 
     // Partition example
     let df = df!("id" => &[1i64, 2, 3], "group" => &["a", "b", "a"])?;
     write_partitioned(&df, &["group"], base.join("partition").to_str().unwrap())?;
 
     // Query example
     let df = df!("id" => &[1i64, 2, 3], "name" => &["alice", "bob", "anne"])?;
     create_and_write_parquet(&df, base.join("query/data.parquet").to_str().unwrap())?;
 
     // XML examples
     let root = crate::xml_examples::parse_sample_xml()?;
     let tables = crate::xml_examples::root_to_tables(&root)?;
     crate::xml_examples::write_tables_to_parquet(&tables, base.join("xml").to_str().unwrap())?;
     std::fs::copy(
         concat!(env!("CARGO_MANIFEST_DIR"), "/tests/fixtures/sample.xml"),
         base.join("xml/sample.xml"),
     )?;
     crate::xml_examples::write_tables_to_parquet(
         &tables,
         base.join("xml_dynamic").to_str().unwrap(),
     )?;
     std::fs::copy(
         concat!(env!("CARGO_MANIFEST_DIR"), "/tests/fixtures/sample.xml"),
         base.join("xml_dynamic/sample.xml"),
     )?;
 
     // Correlation example
     let df = df!(
         "a" => &[1.0f64, 2.0, 3.0],
         "b" => &[1.0f64, 2.0, 3.0],
         "c" => &[3.0f64, 2.0, 1.0]
     )?;
     create_and_write_parquet(&df, base.join("correlation/data.parquet").to_str().unwrap())?;
 
+    // Bundled datasets
+    std::fs::copy(
+        concat!(
+            env!("CARGO_MANIFEST_DIR"),
+            "/tests/fixtures/numeric.parquet"
+        ),
+        base.join("datasets/numeric.parquet"),
+    )?;
+    std::fs::copy(
+        concat!(
+            env!("CARGO_MANIFEST_DIR"),
+            "/tests/fixtures/temporal.parquet"
+        ),
+        base.join("datasets/temporal.parquet"),
+    )?;
+    std::fs::copy(
+        concat!(env!("CARGO_MANIFEST_DIR"), "/tests/fixtures/nested.parquet"),
+        base.join("datasets/nested.parquet"),
+    )?;
+
     Ok(())
 }
 
 #[cfg(test)]
 mod tests {
     use super::*;
     use tempfile::tempdir;
 
     #[test]
     fn round_trip() -> Result<()> {
         let dir = tempdir()?;
         let input_path = dir.path().join("input.parquet");
         let output_path = dir.path().join("output.parquet");
 
         // Create a small DataFrame and write it as Parquet
         let mut df = df!("id" => &[1i64, 2], "name" => &["a", "b"])?;
         write_dataframe_to_parquet(
             &mut df,
             input_path.to_str().unwrap(),
             parquet::basic::Compression::SNAPPY,
         )?;
 
         // Read it back
         let df_read = read_parquet_to_dataframe(input_path.to_str().unwrap())?;
         let mut records = dataframe_to_records(&df_read)?;
diff --git a/tests/fixtures/nested.parquet b/tests/fixtures/nested.parquet
new file mode 100644
index 0000000000000000000000000000000000000000..6334a337d1b015236e6c968b739791f7aca8c1aa
GIT binary patch
literal 920
zcmb7D&ubGw6n;CqPFL|z#TjN`4?*gnxQ%HsP$UK4rdGs8+eK^(Ue<I;14$c`)%;Cf
zJbU)wA0dJl58_|po0*LWM$yOa%=`I$-weC^m~V(kv_x}HZ~<2VR20C)pu@mhSnmSF
zHK9^!TBVmKDqd7n9MnA$ZZ64E@zR=w;{glT;9uFU3zLfYZ*1%jY*+4EwvbtY;TQ0v
z`b8TQpY>16aUmj7af2x~Jb^Y-7lu_io(U%SUWZ27k%~scStXe7S}8~6xIC*sa-HbZ
zKq4aD2EM8-fMAl#Pl5a#nB2twb1gchs{qRabV>%DTU;C9q~I5dEicXqS_mP^SbhuS
z_rSbk=4-H=bouT4PHH&QyTBZ={18fRV)-!yov6#0=T8ss`8!kZ*&S=Q(b|Dq5g#MY
zF&p46;sCn?m(4Q7p7pwjkL*tIVyDQu+1)x%2s;G2Md5=Ufw&<XSl6)-IGHOINBQXG
zk%OlCfyG69@^ilHhJ3%Dw{;IPIeGZJa$)aT&4-yg1c4AZi8{t??sYqG*Gv?r&gt9L
z@%cO*4Y4@12m{=4^L=~X+ML|l-`)n*m=x1jughw^Kbfo-)9LxV@ur;4hUaIE&5dTO
R+0tnOKo7j7Pj{dG$sfLyl*9l4

literal 0
HcmV?d00001

diff --git a/tests/fixtures/numeric.parquet b/tests/fixtures/numeric.parquet
new file mode 100644
index 0000000000000000000000000000000000000000..174d4bf34c1b1325c3304297d09505f07a2de778
GIT binary patch
literal 1012
zcmb7D&2G~`5T13+){PK~D%RSSe8?fSK%^lJEmBaVXq~oz=uaAfz+6ybrD>zK4hbYY
z04Ls{eFk2j7sQn_w~8A)2s7)Lhz5`t**l*3X6E~Llr(i~RoJ2|O51`9SOSnXgGE-e
zAhFEU3lKMjEMid(U+zgKMWn-~H7?xxLl(hofyQxWWeQ-}iZizJGsf(T`l3_#Qom>H
zYki(9MWl0nMs`srvia;)G7vY!5~+qnHDi(iwi=Jc3o=o5P1tIC9V8z`huxF5;HY!i
zk_$)o*Iq^OF^DWh2WbQtIh{B@HlWLur)oZUIAk4oPW|c=mdo8s=6+*-%Xj`~X{v~e
z0$~9jIdrw#T=~FJGVx?no~fdg=aTs!E&m5Yrup6cWn=$l0=3LR%`R)0n(kBoQ>G?=
z-=^|VD@mSdb>G`A_Z|0x4a$?(b*r`a{6Vr}`PsTlcBiM&_US--qV+@vJ-wxMPd5yt
zgFx#Y0vlRCCXaFIF(L>Nv^{gus76Gi@goQhP0~!lzSK;Acv2mAwyOP3Wn%J(ZzfVU
zcG7<R&OlfDA5r#YKP<nf?2bF8BqS9NwGK^05^>lwx(D6|>kY>~iBD{^fp{w2d?xt_
zT`<_~w+_NebwvFe=DyMNmQ;o7K?9i|w1=<XcHgW-gTYFBIE*Lxad$X6icj;E)l#`s
RRy7YGdEh(#)SLK!{|Vb_z6AgP

literal 0
HcmV?d00001

diff --git a/tests/fixtures/temporal.parquet b/tests/fixtures/temporal.parquet
new file mode 100644
index 0000000000000000000000000000000000000000..e70c9f829892bbfd623896ae768c320a59d4a385
GIT binary patch
literal 806
zcma)5PiqrF6o0#2hDCA+!C7Wt4>`1p!nUTtqAe}>wzi8jVzWrvilAwdZfnyfZej}_
zJ@-?1^ymli19<dPh<NZf)1)d@@hvm&-~4{>y<v8nvqkAiCBudkk}5={xRny^VC`Z_
zk~gJ_kt<_~P<bY*JaZ<XTfcZjbYs@VE*y7emOxsQcc-N4IR)Qd4jiP5>VegtHjj<I
zR=}r8M)^|=3jHB^t)O0AA|*zh!`?|-dL-RBSNYxcxc8zn=+BiJeITTVV95l>M<<tw
zO5>suhR9(ep+g-<6iQbwktpk*c<BZrc<%%soajO1|LY!fjM-7$j8=wPy)AS^0`!L1
zn-KrjgC+m$1Yex!og000Ld^cZxA6CJ!FTi=`N72u=}?t8%bx74Twag*>fAAMkBv0#
z&b-BZ&PmRj%sH>;Hku^NkBzm?{K#ndQw`6UiOnO9lW#$KAI8SkhVcP{SkqeI$NA&E
z-K^hD`{QS=<yZT46GP=QK#keNmoyDDW$y4plLYw{Ni(<o+9s}ApOi|o!*1F-*sCv(
zfC4@@^FU8!ZH-iQ*e+hb>5b<*!{L0pC<f=%vtBVe8k|<^iwm`dnvU@t%!_!6_q>fi
F<Oc*0pY#9#

literal 0
HcmV?d00001

